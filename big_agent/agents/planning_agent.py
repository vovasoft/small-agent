
from typing import List, Dict, Optional, Any, Union
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
import json
import os
from datetime import datetime


# æ•°æ®æ¨¡å‹å®šä¹‰
class ActionItem(BaseModel):
    """åŠ¨ä½œé¡¹å®šä¹‰"""
    action: str = Field(description="åŠ¨ä½œåç§°")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict, description="åŠ¨ä½œå‚æ•°")


class ClarificationRequest(BaseModel):
    """æ¾„æ¸…è¯·æ±‚ç»“æ„åŒ–æ ¼å¼"""
    questions: List[str] = Field(description="éœ€è¦æ¾„æ¸…çš„é—®é¢˜åˆ—è¡¨")
    missing_fields: List[str] = Field(default_factory=list, description="ç¼ºå°‘çš„å­—æ®µæˆ–ä¿¡æ¯")


class PlanningDecision(BaseModel):
    """è§„åˆ’å†³ç­–è¾“å‡º"""
    decision: str = Field(
        description="å†³ç­–ç±»å‹: generate_outline, compute_metrics, finalize_report, clarify_requirements"
    )
    reasoning: str = Field(description="è¯¦ç»†æ¨ç†è¿‡ç¨‹")
    next_actions: List[Union[str, ActionItem]] = Field(
        default_factory=list,
        description="ä¸‹ä¸€æ­¥åŠ¨ä½œåˆ—è¡¨"
    )
    metrics_to_compute: List[str] = Field(
        default_factory=list,
        description="å¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨ï¼ˆå¦‚ ['total_income', 'avg_balance']ï¼‰"
    )
    priority_metrics: List[str] = Field(
        default_factory=list,
        description="ä¼˜å…ˆçº§é«˜çš„æŒ‡æ ‡ID"
    )
    additional_requirements: Optional[
        Union[Dict[str, Any], List[Any], ClarificationRequest]
    ] = Field(default=None, description="é¢å¤–éœ€æ±‚æˆ–æ¾„æ¸…ä¿¡æ¯")


def normalize_requirements(req: Any) -> Optional[Dict[str, Any]]:
    """
    è§„èŒƒåŒ– additional_requirements
    å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    """
    if req is None:
        return None

    if isinstance(req, dict):
        return req

    if isinstance(req, list):
        # å¦‚æœLLMé”™è¯¯åœ°è¿”å›äº†åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        return {
            "questions": [str(item) for item in req],
            "missing_fields": []
        }

    return {"raw": str(req)}


class PlanningAgent:
    """è§„åˆ’æ™ºèƒ½ä½“ï¼šè´Ÿè´£çŠ¶æ€åˆ†æå’Œå†³ç­–åˆ¶å®š"""

    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com"):
        """
        åˆå§‹åŒ–è§„åˆ’Agent

        Args:
            api_key: DeepSeek APIå¯†é’¥
            base_url: DeepSeek APIåŸºç¡€URL
        """
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            api_key=api_key,
            base_url=base_url,
            temperature=0.1
        )

        # åˆå§‹åŒ–APIè°ƒç”¨è·Ÿè¸ª
        self.api_calls = []

    def create_planning_prompt(self) -> ChatPromptTemplate:
        """åˆ›å»ºè§„åˆ’æç¤ºæ¨¡æ¿"""
        return ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æŠ¥å‘Šè§„åˆ’æ€»æ§æ™ºèƒ½ä½“ï¼Œæ ¸å¿ƒèŒè´£æ˜¯ç²¾å‡†åˆ†æå½“å‰çŠ¶æ€å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚

### å†³ç­–é€‰é¡¹ï¼ˆäºŒé€‰ä¸€ï¼‰
1. generate_outlineï¼šå¤§çº²æœªç”Ÿæˆæˆ–å¤§çº²æ— æ•ˆ
2. compute_metricsï¼šå¤§çº²å·²ç”Ÿæˆä½†æŒ‡æ ‡æœªå®Œæˆ

### å†³ç­–è§„åˆ™ï¼ˆæŒ‰é¡ºåºæ£€æŸ¥ï¼‰
1. æ£€æŸ¥ outline_draft æ˜¯å¦ä¸ºç©º â†’ ç©ºåˆ™é€‰æ‹© generate_outline
2. æ£€æŸ¥ metrics_requirements æ˜¯å¦ä¸ºç©º â†’ ç©ºåˆ™é€‰æ‹© generate_outline
3. æ£€æŸ¥æ˜¯å¦æœ‰å¾…è®¡ç®—æŒ‡æ ‡ â†’ æœ‰åˆ™é€‰æ‹© compute_metrics
4. æ‰€æœ‰æŒ‡æ ‡éƒ½å·²è®¡ç®—å®Œæˆ â†’ é€‰æ‹© finalize_report
5. å¦‚æœæ— æ³•ç†è§£éœ€æ±‚ â†’ é€‰æ‹© clarify_requirements

### é‡è¦åŸåˆ™
- å¤§çº²è‰ç¨¿å·²å­˜åœ¨æ—¶ï¼Œä¸è¦é‡å¤ç”Ÿæˆå¤§çº²
- å†³ç­–ä¸º compute_metrics æ—¶ï¼Œå¿…é¡»ä»çŠ¶æ€ä¿¡æ¯ä¸­çš„"æœ‰æ•ˆå¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨"ä¸­é€‰æ‹©
- ç¡®ä¿ metrics_to_compute æ˜¯å­—ç¬¦ä¸²æ•°ç»„æ ¼å¼
- ç¡®ä¿æŒ‡æ ‡IDä¸å¤§çº²ä¸­çš„global_metrics.metric_idå®Œå…¨ä¸€è‡´
- ä»çŠ¶æ€ä¿¡æ¯ä¸­çš„"æœ‰æ•ˆå¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨"ä¸­æå–metric_idä½œä¸ºmetrics_to_computeçš„å€¼
- è®¡ç®—å¤±è´¥çš„æŒ‡æ ‡å¯ä»¥é‡è¯•æœ€å¤š3æ¬¡
- ç»å¯¹ä¸è¦è‡ªå·±ç”Ÿæˆæ–°çš„æŒ‡æ ‡IDï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨çŠ¶æ€ä¿¡æ¯ä¸­æä¾›çš„å·²æœ‰æŒ‡æ ‡ID
- å¦‚æœçŠ¶æ€ä¿¡æ¯ä¸­æ²¡æœ‰å¯ç”¨çš„æŒ‡æ ‡IDï¼Œä¸è¦ç”Ÿæˆcompute_metricså†³ç­–

### è¾“å‡ºå­—æ®µè¯´æ˜
- decision: å†³ç­–å­—ç¬¦ä¸²
- reasoning: å†³ç­–åŸå› è¯´æ˜
- metrics_to_compute: å¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨ï¼Œå¿…é¡»ä»çŠ¶æ€ä¿¡æ¯ä¸­çš„"æœ‰æ•ˆå¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨"ä¸­é€‰æ‹©ã€‚é€‰æ‹©æ‰€æœ‰å¯ç”¨æŒ‡æ ‡ï¼Œé™¤éæŒ‡æ ‡æ•°é‡è¿‡å¤šï¼ˆ>10ä¸ªï¼‰éœ€è¦åˆ†æ‰¹è®¡ç®—
- priority_metrics: ä¼˜å…ˆçº§æŒ‡æ ‡åˆ—è¡¨ï¼ˆå‰2-3ä¸ªæœ€é‡è¦çš„æŒ‡æ ‡ï¼‰ï¼Œä»metrics_to_computeä¸­é€‰æ‹©

å¿…é¡»è¾“å‡ºæœ‰æ•ˆçš„JSONæ ¼å¼ï¼"""),

            MessagesPlaceholder("messages"),

            ("user", "æŠ¥å‘Šéœ€æ±‚ï¼š{question}\n\nè¯·è¾“å‡ºå†³ç­–ç»“æœã€‚")
        ])

    async def make_decision(self, question: str, industry: str, current_state: Dict[str, Any]) -> PlanningDecision:
        """
        æ ¹æ®å½“å‰çŠ¶æ€åšå‡ºè§„åˆ’å†³ç­–

        Args:
            question: ç”¨æˆ·æŸ¥è¯¢
            industry: è¡Œä¸š
            current_state: å½“å‰çŠ¶æ€ä¿¡æ¯

        Returns:
            è§„åˆ’å†³ç­–ç»“æœ
        """
        planner = self.create_planning_prompt() | self.llm

        # æ„å»ºçŠ¶æ€è¯„ä¼°ä¸Šä¸‹æ–‡
        status_info = self._build_status_context(current_state)

        # è®°å½•å¤§æ¨¡å‹è¾“å…¥
        print("========================================")
        print("[AGENT] PlanningAgent (è§„åˆ’Agent)")
        print("[MODEL_INPUT] PlanningAgent:")
        print(f"[CONTEXT] åŸºäºå½“å‰çŠ¶æ€åšå‡ºè§„åˆ’å†³ç­–")
        print(f"Question: {question}")
        print(f"Status info: {status_info}")
        print("========================================")

        # æ‰§è¡Œè§„åˆ’
        start_time = datetime.now()
        response = await planner.ainvoke({
            "question": question,
            "industry": industry,
            "messages": [("system", status_info)]
        })
        end_time = datetime.now()

        # è§£æJSONå“åº”
        try:
            # ä»å“åº”ä¸­æå–JSONå†…å®¹
            content = response.content if hasattr(response, 'content') else str(response)
            # å°è¯•æ‰¾åˆ°JSONéƒ¨åˆ†
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = content[json_start:json_end]
                decision_data = json.loads(json_str)

                # é¢„å¤„ç† additional_requirements å­—æ®µ
                if "additional_requirements" in decision_data:
                    req = decision_data["additional_requirements"]
                    if isinstance(req, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•å°†å…¶è½¬æ¢ä¸ºåˆé€‚çš„æ ¼å¼
                        if req.strip():
                            # å°†å­—ç¬¦ä¸²åŒ…è£…ä¸ºå­—å…¸æ ¼å¼
                            decision_data["additional_requirements"] = {"raw_content": req}
                        else:
                            # ç©ºå­—ç¬¦ä¸²è®¾ä¸º None
                            decision_data["additional_requirements"] = None
                    elif isinstance(req, list):
                        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                        decision_data["additional_requirements"] = {
                            "questions": [str(item) for item in req],
                            "missing_fields": []
                        }
                    # å¦‚æœå·²ç»æ˜¯ dict æˆ–å…¶ä»–å…è®¸çš„ç±»å‹ï¼Œä¿æŒä¸å˜

                decision = PlanningDecision(**decision_data)

                # éªŒè¯å†³ç­–çš„åˆç†æ€§
                if decision.decision == "compute_metrics":
                    if not decision.metrics_to_compute:
                        raise ValueError("AIå†³ç­–ç¼ºå°‘å…·ä½“çš„æŒ‡æ ‡ID")
                    # å¦‚æœAIç”Ÿæˆçš„æŒ‡æ ‡IDæ˜æ˜¾æ˜¯é”™è¯¯çš„ï¼ˆæ¯”å¦‚metric_001ï¼‰ï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘
                    if any(mid.startswith("metric_") and mid.replace("metric_", "").isdigit()
                          for mid in decision.metrics_to_compute):
                        raise ValueError("AIç”Ÿæˆçš„æŒ‡æ ‡IDæ ¼å¼ä¸æ­£ç¡®")

            else:
                raise ValueError("No JSON found in response")
        except Exception as e:
            print(f"è§£æè§„åˆ’å†³ç­–å“åº”å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å†³ç­–")
            # è¿”å›é»˜è®¤å†³ç­–
            decision = self._get_default_decision(current_state)

        # è®°å½•APIè°ƒç”¨ç»“æœ
        content = response.content if hasattr(response, 'content') else str(response)
        call_id = f"api_mll_è§„åˆ’å†³ç­–_{'{:.2f}'.format((end_time - start_time).total_seconds())}"
        api_call_info = {
            "call_id": call_id,
            "timestamp": end_time.isoformat(),
            "agent": "PlanningAgent",
            "model": "deepseek-chat",
            "request": {
                "question": question,
                "status_info": status_info,
                "start_time": start_time.isoformat()
            },
            "response": {
                "content": content,
                "decision": decision.dict() if hasattr(decision, 'dict') else decision,
                "end_time": end_time.isoformat(),
                "duration": (end_time - start_time).total_seconds()
            },
            "success": True
        }
        self.api_calls.append(api_call_info)

        # ä¿å­˜APIç»“æœåˆ°æ–‡ä»¶
        api_results_dir = "api_results"
        os.makedirs(api_results_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{call_id}.json"
        filepath = os.path.join(api_results_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(api_call_info, f, ensure_ascii=False, indent=2)
            print(f"[API_RESULT] ä¿å­˜APIç»“æœæ–‡ä»¶: {filepath}")
        except Exception as e:
            print(f"[ERROR] ä¿å­˜APIç»“æœæ–‡ä»¶å¤±è´¥: {filepath}, é”™è¯¯: {str(e)}")

        # è®°å½•å¤§æ¨¡å‹è¾“å‡º
        print(f"[MODEL_OUTPUT] PlanningAgent: {json.dumps(decision.dict() if hasattr(decision, 'dict') else decision, ensure_ascii=False)}")
        print("========================================")

        return decision

    def _build_status_context(self, state: Dict[str, Any]) -> str:
        """æ„å»ºçŠ¶æ€è¯„ä¼°ä¸Šä¸‹æ–‡"""
        required_count = len(state.get("metrics_requirements", []))
        computed_count = len(state.get("computed_metrics", {}))
        coverage = computed_count / required_count if required_count > 0 else 0

        # è®¡ç®—å¤±è´¥ç»Ÿè®¡
        failed_attempts = state.get("failed_metric_attempts", {})
        pending_ids = state.get("pending_metric_ids", [])

        # è¿‡æ»¤æ‰å¤±è´¥æ¬¡æ•°è¿‡å¤šçš„æŒ‡æ ‡
        max_retry = 3
        filtered_pending_ids = [
            mid for mid in pending_ids
            if failed_attempts.get(mid, 0) < max_retry
        ]

        # è·å–å¯ç”¨çš„æŒ‡æ ‡ID
        available_metric_ids = []
        outline_draft = state.get('outline_draft')
        if outline_draft and outline_draft.global_metrics:
            available_metric_ids = [m.metric_id for m in outline_draft.global_metrics if m.metric_id]
        

        return f"""å½“å‰çŠ¶æ€è¯„ä¼°ï¼š
- è§„åˆ’æ­¥éª¤: {state.get('planning_step', 0)}
- å¤§çº²ç‰ˆæœ¬: {state.get('outline_version', 0)}
- å¤§çº²è‰ç¨¿å­˜åœ¨: {state.get('outline_draft') is not None}
- æŒ‡æ ‡éœ€æ±‚æ€»æ•°: {required_count}
- å·²è®¡ç®—æŒ‡æ ‡æ•°: {computed_count}
- æŒ‡æ ‡è¦†ç›–ç‡: {coverage:.2%}
- å¾…è®¡ç®—æŒ‡æ ‡æ•°: {len(pending_ids)}
- æœ‰æ•ˆå¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨: {filtered_pending_ids}
- å¯ç”¨æŒ‡æ ‡IDåˆ—è¡¨: {available_metric_ids}
- å¤±è´¥å°è¯•è®°å½•: {failed_attempts}
"""


def analyze_current_state(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†æå½“å‰çŠ¶æ€ï¼Œè¿”å›å…³é”®ä¿¡æ¯

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        çŠ¶æ€åˆ†æç»“æœ
    """
    required_metrics = state.get("metrics_requirements", [])
    computed_metrics = state.get("computed_metrics", {})

    # è®¡ç®—è¦†ç›–ç‡
    required_count = len(required_metrics)
    computed_count = len(computed_metrics)
    coverage = computed_count / required_count if required_count > 0 else 0

    # æ‰¾å‡ºæœªè®¡ç®—çš„æŒ‡æ ‡
    computed_ids = set(computed_metrics.keys())
    pending_metrics = [
        m for m in required_metrics
        if m.metric_id not in computed_ids
    ]

    # æ£€æŸ¥å¤±è´¥æ¬¡æ•°
    failed_attempts = state.get("failed_metric_attempts", {})
    max_retry = 3
    valid_pending_metrics = [
        m for m in pending_metrics
        if failed_attempts.get(m.metric_id, 0) < max_retry
    ]

    return {
        "has_outline": state.get("outline_draft") is not None,
        "required_count": required_count,
        "computed_count": computed_count,
        "coverage": coverage,
        "pending_metrics": pending_metrics,
        "valid_pending_metrics": valid_pending_metrics,
        "pending_ids": [m.metric_id for m in pending_metrics],
        "valid_pending_ids": [m.metric_id for m in valid_pending_metrics],
        "planning_step": state.get("planning_step", 0),
        "outline_version": state.get("outline_version", 0)
    }


async def plan_next_action(question: str, industry: str, current_state: Dict[str, Any], api_key: str) -> PlanningDecision:
    """
    è§„åˆ’ä¸‹ä¸€æ­¥è¡ŒåŠ¨çš„ä¸»å‡½æ•°

    Args:
        question: ç”¨æˆ·æŸ¥è¯¢
        current_state: å½“å‰çŠ¶æ€
        api_key: APIå¯†é’¥

    Returns:
        è§„åˆ’å†³ç­–ç»“æœ
    """
    agent = PlanningAgent(api_key)

    try:
        decision = await agent.make_decision(question, industry, current_state)

        print(f"\nğŸ§  è§„åˆ’å†³ç­–ï¼š{decision.decision}")
        print(f"   æ¨ç†ï¼š{decision.reasoning[:100]}...")

        if decision.metrics_to_compute:
            print(f"   å¾…è®¡ç®—æŒ‡æ ‡ï¼š{decision.metrics_to_compute}")

        return decision

    except Exception as e:
        print(f"âš ï¸ è§„åˆ’å†³ç­–å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤å†³ç­–")

        # ç›´æ¥è¿”å›æœ€åŸºæœ¬çš„é»˜è®¤å†³ç­–ï¼Œé¿å…å¤æ‚çš„é»˜è®¤å†³ç­–é€»è¾‘
        return PlanningDecision(
            decision="finalize_report",
            reasoning="è§„åˆ’å†³ç­–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çš„æŠ¥å‘Šç”Ÿæˆå†³ç­–",
            next_actions=["ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"],
            metrics_to_compute=[],
            priority_metrics=[]
        )

   