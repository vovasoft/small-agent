from typing import List, Dict, Optional, Any, Union
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
import json

from llmops.agents.state import AgentState, MetricRequirement, convert_numpy_types
from llmops.agents.datadev.llm import get_llm


class ActionItem(BaseModel):
    """åŠ¨ä½œé¡¹å®šä¹‰"""
    action: str = Field(description="åŠ¨ä½œåç§°")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict)


class ClarificationRequest(BaseModel):
    """æ¾„æ¸…è¯·æ±‚ç»“æ„åŒ–æ ¼å¼"""
    questions: List[str] = Field(description="éœ€è¦æ¾„æ¸…çš„é—®é¢˜åˆ—è¡¨")
    missing_fields: List[str] = Field(default_factory=list, description="ç¼ºå°‘çš„å­—æ®µæˆ–ä¿¡æ¯")


class PlanningOutput(BaseModel):
    """è§„åˆ’å†³ç­–è¾“å‡º - æ”¯æŒçµæ´»æ ¼å¼"""
    decision: str = Field(
        description="å†³ç­–ç±»å‹: generate_outline, compute_metrics, finalize, clarify"
    )
    reasoning: str = Field(description="è¯¦ç»†æ¨ç†è¿‡ç¨‹")
    next_actions: List[Union[str, ActionItem]] = Field(
        default_factory=list,
        description="ä¸‹ä¸€æ­¥åŠ¨ä½œåˆ—è¡¨"
    )
    # å…³é”®ä¿®å¤ï¼šæ˜ç¡®ä¼ é€’å¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨
    metrics_to_compute: List[str] = Field(
        default_factory=list,
        description="å¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨ï¼ˆå¦‚ ['total_income', 'avg_balance']ï¼‰"
    )
    additional_requirements: Optional[
        Union[Dict[str, Any], List[Any], ClarificationRequest]
    ] = Field(default=None, description="é¢å¤–éœ€æ±‚æˆ–æ¾„æ¸…ä¿¡æ¯")


def normalize_additional_requirements(req: Any) -> Optional[Dict[str, Any]]:
    """
    è§„èŒƒåŒ– additional_requirements
    å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    """
    if req is None:
        return None

    if isinstance(req, dict):
        return req

    if isinstance(req, list):
        # å¦‚æœLLMé”™è¯¯åœ°è¿”å›äº†åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        return {
            "questions": [str(item) for item in req],
            "missing_fields": []
        }

    return {"raw": str(req)}


def create_planning_agent(llm, state: AgentState):
    """åˆ›å»ºè§„åˆ’æ™ºèƒ½ä½“ï¼ˆä¿®å¤ç‰ˆï¼šç§»é™¤JSONç¤ºä¾‹ï¼Œé¿å…å˜é‡å†²çªï¼‰"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯æŠ¥å‘Šè§„åˆ’æ€»æ§æ™ºèƒ½ä½“ï¼Œæ ¸å¿ƒèŒè´£æ˜¯ç²¾å‡†åˆ†æå½“å‰çŠ¶æ€å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚

### å†³ç­–é€‰é¡¹ï¼ˆå››é€‰ä¸€ï¼‰
1. generate_outlineï¼šå¤§çº²æœªç”Ÿæˆæˆ–å¤§çº²æ— æ•ˆ
2. compute_metricsï¼šå¤§çº²å·²ç”Ÿæˆä½†æŒ‡æ ‡æœªå®Œæˆï¼ˆè¦†ç›–ç‡<80%ï¼‰
3. finalizeï¼šæŒ‡æ ‡è¦†ç›–ç‡â‰¥80%ï¼Œä¿¡æ¯å……è¶³
4. clarifyï¼šç”¨æˆ·éœ€æ±‚æ¨¡ç³Šï¼Œç¼ºå°‘å…³é”®ä¿¡æ¯

### å†³ç­–è§„åˆ™ï¼ˆæŒ‰é¡ºåºæ£€æŸ¥ï¼‰
1. æ£€æŸ¥ outline_draft æ˜¯å¦ä¸ºç©º â†’ ç©ºåˆ™é€‰æ‹© generate_outline
2. æ£€æŸ¥ metrics_requirements æ˜¯å¦ä¸ºç©º â†’ ç©ºåˆ™é€‰æ‹© generate_outline
3. è®¡ç®—æŒ‡æ ‡è¦†ç›–ç‡ = å·²è®¡ç®—æŒ‡æ ‡ / æ€»éœ€æ±‚æŒ‡æ ‡
   - è¦†ç›–ç‡ < 0.8 â†’ é€‰æ‹© compute_metrics
   - è¦†ç›–ç‡ â‰¥ 0.8 â†’ é€‰æ‹© finalize
4. å¦‚æœæ— æ³•ç†è§£éœ€æ±‚ â†’ é€‰æ‹© clarify

### é‡è¦åŸåˆ™
- å¤§çº²è‰ç¨¿å·²å­˜åœ¨æ—¶ï¼Œä¸è¦é‡å¤ç”Ÿæˆå¤§çº²
- å†³ç­–ä¸º compute_metrics æ—¶ï¼Œå¿…é¡»æä¾›å…·ä½“çš„æŒ‡æ ‡IDåˆ—è¡¨
- ç¡®ä¿ metrics_to_compute æ˜¯å­—ç¬¦ä¸²æ•°ç»„æ ¼å¼

### è¾“å‡ºå­—æ®µè¯´æ˜
- decision: å†³ç­–å­—ç¬¦ä¸²
- reasoning: å†³ç­–åŸå› è¯´æ˜
- next_actions: åŠ¨ä½œåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
- metrics_to_compute: å¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨ï¼ˆå†³ç­–ä¸ºcompute_metricsæ—¶å¿…é¡»æä¾›ï¼‰
- additional_requirements: é¢å¤–éœ€æ±‚ï¼ˆå¯é€‰ï¼‰

å¿…é¡»è¾“å‡ºæœ‰æ•ˆçš„JSONæ ¼å¼ï¼"""),

    MessagesPlaceholder("messages"),

    ("user", "æŠ¥å‘Šéœ€æ±‚ï¼š{question}\n\nè¯·è¾“å‡ºå†³ç­–ç»“æœã€‚")
    ])

    return prompt | llm.with_structured_output(PlanningOutput)


async def planning_node(state: AgentState) -> AgentState:
    """è§„åˆ’èŠ‚ç‚¹ï¼šæ­£ç¡®è¯†åˆ«å¾…è®¡ç®—æŒ‡æ ‡å¹¶ä¼ é€’"""
    llm = get_llm()
    planner = create_planning_agent(llm, state)

    # æ„å»ºå®Œæ•´çš„çŠ¶æ€è¯„ä¼°ä¸Šä¸‹æ–‡
    required_count = len(state["metrics_requirements"])
    computed_count = len(state["computed_metrics"])
    coverage = computed_count / required_count if required_count > 0 else 0

    # æ–°å¢ï¼šè·Ÿè¸ªå¤±è´¥æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
    failed_attempts = state.get("failed_metric_attempts", {})
    pending_ids = state.get("pending_metric_ids", [])

    # è¿‡æ»¤æ‰å¤±è´¥æ¬¡æ•°è¿‡å¤šçš„æŒ‡æ ‡
    max_retry = 3
    filtered_pending_ids = [
        mid for mid in pending_ids
        if failed_attempts.get(mid, 0) < max_retry
    ]

    status_snapshot = f"""å½“å‰çŠ¶æ€è¯„ä¼°ï¼š
- è§„åˆ’æ­¥éª¤: {state['planning_step']}
- å¤§çº²ç‰ˆæœ¬: {state['outline_version']}
- å¤§çº²è‰ç¨¿å­˜åœ¨: {state['outline_draft'] is not None}
- æŒ‡æ ‡éœ€æ±‚æ€»æ•°: {required_count}
- å·²è®¡ç®—æŒ‡æ ‡æ•°: {computed_count}
- æŒ‡æ ‡è¦†ç›–ç‡: {coverage:.2%}
- å¾…è®¡ç®—æŒ‡æ ‡æ•°: {len(pending_ids)}
- æœ‰æ•ˆå¾…è®¡ç®—æŒ‡æ ‡æ•°: {len(filtered_pending_ids)}
- å¤±è´¥å°è¯•è®°å½•: {failed_attempts}

å»ºè®®ä¸‹ä¸€æ­¥: {"è®¡ç®—æŒ‡æ ‡" if coverage < 0.8 else "ç”ŸæˆæŠ¥å‘Š"}"""

    # æ‰§è¡Œè§„åˆ’
    result = await planner.ainvoke({
        "question": state["question"],
        "messages": [("system", status_snapshot)]
    })

    # è§„èŒƒåŒ–ç»“æœ
    normalized_req = normalize_additional_requirements(result.additional_requirements)

    # æ‰¾å‡ºæ‰€æœ‰æœªè®¡ç®—çš„æŒ‡æ ‡
    computed_ids = set(state["computed_metrics"].keys())
    required_metrics = state["metrics_requirements"]

    pending_metrics = [
        m for m in required_metrics
        if m.metric_id not in computed_ids
    ]

    # å…³é”®ï¼šä½¿ç”¨ LLM è¿”å›çš„æŒ‡æ ‡IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å…¨éƒ¨å¾…è®¡ç®—æŒ‡æ ‡
    if result.metrics_to_compute:
        pending_ids = result.metrics_to_compute
        valid_ids = [m.metric_id for m in pending_metrics]
        pending_metrics = [m for m in pending_metrics if m.metric_id in pending_ids and m.metric_id in valid_ids]

    # æ›´æ–°çŠ¶æ€
    new_state = state.copy()
    new_state["plan_history"].append(
        f"Step {new_state['planning_step']}: {result.decision}"
    )
    new_state["planning_step"] += 1
    new_state["additional_requirements"] = normalized_req

    # å…³é”®ï¼šä¿å­˜å¾…è®¡ç®—æŒ‡æ ‡IDåˆ—è¡¨
    if pending_metrics:
        pending_ids = [m.metric_id for m in pending_metrics]
        new_state["pending_metric_ids"] = pending_ids
        new_state["metrics_to_compute"] = pending_metrics  # ä¿å­˜å®Œæ•´å¯¹è±¡

    # è®¾ç½®è·¯ç”±æ ‡å¿—
    if result.decision == "generate_outline":
        new_state["messages"].append(
            ("ai", f"ğŸ“‹ è§„åˆ’å†³ç­–ï¼šç”Ÿæˆå¤§çº² (v{new_state['outline_version'] + 1})")
        )
        new_state["next_route"] = "outline_generator"
    elif result.decision == "compute_metrics":
        # ä¿®å¤ï¼šç¡®ä¿æ˜¾ç¤ºæ­£ç¡®çš„æ•°é‡
        if not pending_metrics:
            # å¦‚æœæ²¡æœ‰å¾…è®¡ç®—æŒ‡æ ‡ä½†æœ‰éœ€æ±‚ï¼Œåˆ™è®¡ç®—æ‰€æœ‰æœªå®Œæˆçš„
            computed_ids = set(state["computed_metrics"].keys())
            pending_metrics = [m for m in required_metrics if m.metric_id not in computed_ids]

        # æ–°å¢ï¼šå¦‚æœæœ‰æ•ˆå¾…è®¡ç®—æŒ‡æ ‡ä¸ºç©ºä½†è¿˜æœ‰æŒ‡æ ‡æœªè®¡ç®—ï¼Œè¯´æ˜éƒ½å¤±è´¥äº†å¤ªå¤šæ¬¡
        if not filtered_pending_ids and pending_ids:
            new_state["messages"].append(
                ("ai", f"âš ï¸ å‰©ä½™ {len(pending_ids)} ä¸ªæŒ‡æ ‡å·²å¤šæ¬¡è®¡ç®—å¤±è´¥ï¼Œå°†è·³è¿‡è¿™äº›æŒ‡æ ‡ç›´æ¥ç”ŸæˆæŠ¥å‘Š")
            )
            new_state["next_route"] = "report_compiler"
            # å…³é”®ä¿®å¤ï¼šè¿”å›å‰æ¸…ç†çŠ¶æ€
            return convert_numpy_types(new_state)

        new_state["messages"].append(
            ("ai", f"ğŸ§® è§„åˆ’å†³ç­–ï¼šè®¡ç®— {len(pending_metrics)} ä¸ªæŒ‡æ ‡ ({[m.metric_id for m in pending_metrics]})")
        )
        new_state["next_route"] = "metrics_calculator"
    elif result.decision == "finalize":
        new_state["is_complete"] = True
        new_state["messages"].append(
            ("ai", f"âœ… è§„åˆ’å†³ç­–ï¼šä¿¡æ¯å……è¶³ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆè¦†ç›–ç‡ {coverage:.2%}ï¼‰")
        )
        new_state["next_route"] = "report_compiler"
    elif result.decision == "clarify":
        questions = []
        if normalized_req and "questions" in normalized_req:
            questions = normalized_req["questions"]

        new_state["messages"].append(
            ("ai", f"â“ éœ€è¦æ¾„æ¸…ï¼š{'ï¼›'.join(questions) if questions else 'è¯·æä¾›æ›´è¯¦ç»†çš„éœ€æ±‚'}")
        )
        new_state["next_route"] = "clarify_node"

    # å…³é”®ä¿®å¤ï¼šè¿”å›å‰æ¸…ç†çŠ¶æ€
    return convert_numpy_types(new_state)