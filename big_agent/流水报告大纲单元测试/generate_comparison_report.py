"""
æµæ°´åˆ†ææŠ¥å‘Šå¤§çº²æ¯”è¾ƒæŠ¥å‘Šç”Ÿæˆå™¨
================================

æ­¤è„šæœ¬ä¸“é—¨ç”¨äºç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Šï¼Œä¸é‡æ–°ç”Ÿæˆmdæ–‡ä»¶ï¼Œç›´æ¥åˆ†æå·²æœ‰çš„30ä¸ªmdæ–‡ä»¶ã€‚

æµ‹è¯•æµç¨‹ï¼š
1. è¯»å–å·²æœ‰çš„level1-1.mdåˆ°level3-10.mdæ–‡ä»¶
2. å°†ç»“æœè½¬æ¢ä¸ºåŸºç¡€ç‰ˆæœ¬æ ¼å¼
3. è¿›è¡Œå¤šç»´åº¦æ¯”è¾ƒåˆ†æï¼ˆä½¿ç”¨å¤§æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦ï¼‰
4. ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š

ä½œè€…: Test Team
ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¶é—´: 2024-12-20
"""

import os
import json
from typing import List, Dict, Any
from datetime import datetime
from pathlib import Path
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import re


class ComparisonReportGenerator:
    """æ¯”è¾ƒæŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com"):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨"""
        self.api_key = "sk-438668d443224063adbb1d295fe44a9f"
        self.base_url = base_url
        self.test_dir = Path("æµæ°´æŠ¥å‘Šå¤§çº²å•å…ƒæµ‹è¯•")

        # åˆå§‹åŒ–DeepSeekæ¨¡å‹
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            api_key=api_key,
            base_url=base_url,
            temperature=0.1
        )

        # åŠ è½½åŸºç¡€ç‰ˆæœ¬
        self.baseline_content = self._load_baseline()

    def _load_baseline(self) -> str:
        """åŠ è½½åŸºç¡€ç‰ˆæœ¬å†…å®¹"""
        baseline_file = self.test_dir / "æµæ°´åˆ†æå¤§çº²-åŸºç¡€ç‰ˆæœ¬.md"
        with open(baseline_file, 'r', encoding='utf-8') as f:
            return f.read().strip()

    def _convert_to_baseline_format(self, content: str) -> str:
        """å°†å†…å®¹è½¬æ¢ä¸ºåŸºç¡€ç‰ˆæœ¬æ ¼å¼ï¼ˆåªæœ‰æ ‡é¢˜å’Œç¼©è¿›ï¼‰"""
        lines = content.split('\n')
        converted_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ä¿ç•™æ ‡é¢˜ç»“æ„
            if line.startswith('#'):
                converted_lines.append(line)
            # ä¿ç•™ç¼©è¿›ç»“æ„ï¼ˆè½¬æ¢ä¸ºmarkdownåˆ—è¡¨ï¼‰
            elif line.startswith('-') or line.startswith('*'):
                converted_lines.append(line)
            # è½¬æ¢å…¶ä»–å†…å®¹ä¸ºåˆ—è¡¨é¡¹
            elif len(line) > 0:
                converted_lines.append(f"- {line}")

        return '\n'.join(converted_lines)

    async def _calculate_similarity(self, text1: str, text2: str) -> float:
        """ä½¿ç”¨å¤§æ¨¡å‹è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œç»™å‡ºä¸€ä¸ª0-1ä¹‹é—´çš„åˆ†æ•°ã€‚

æ–‡æœ¬1ï¼š
{text1}

æ–‡æœ¬2ï¼š
{text2}

è¦æ±‚ï¼š
1. åˆ†æä¸¤ä¸ªæ–‡æœ¬åœ¨è¯­ä¹‰ã€ç»“æ„ã€å†…å®¹æ–¹é¢çš„ç›¸ä¼¼ç¨‹åº¦
2. è€ƒè™‘ä¸“ä¸šæœ¯è¯­ã€æ¦‚å¿µä¸€è‡´æ€§ã€é€»è¾‘ç»“æ„ç­‰å› ç´ 
3. è¾“å‡ºä¸€ä¸ª0-1ä¹‹é—´çš„å°æ•°ï¼Œ1.0è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ0.0è¡¨ç¤ºå®Œå…¨ä¸åŒ
4. åªè¾“å‡ºæ•°å­—ï¼Œä¸è¦å…¶ä»–æ–‡å­—

ç›¸ä¼¼åº¦è¯„åˆ†ï¼š"""

        try:
            messages = [
                ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æä¸“å®¶ï¼Œåªè¾“å‡º0-1ä¹‹é—´çš„æ•°å­—ã€‚"),
                ("user", prompt)
            ]

            response = await self.llm.ainvoke(messages)
            content = response.content if hasattr(response, 'content') else str(response)

            # æå–æ•°å­—
            import re
            match = re.search(r'(\d+\.?\d*)', content.strip())
            if match:
                similarity = float(match.group(1))
                # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                return max(0.0, min(1.0, similarity))
            else:
                print(f"æ— æ³•è§£æç›¸ä¼¼åº¦ç»“æœ: {content}")
                return 0.0

        except Exception as e:
            print(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            # è¿”å›ç®€å•çš„Jaccardç›¸ä¼¼åº¦ä½œä¸ºfallback
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())

            if not words1 and not words2:
                return 1.0

            intersection = words1.intersection(words2)
            union = words1.union(words2)

            return len(intersection) / len(union) if union else 0.0

    def _extract_sections(self, content: str) -> List[Dict[str, str]]:
        """æå–å†…å®¹ä¸­çš„ç« èŠ‚"""
        lines = content.split('\n')
        sections = []

        for line in lines:
            if line.startswith('#'):
                # ç§»é™¤#å·å’Œç©ºæ ¼
                title = line.lstrip('#').strip()
                sections.append({'title': title, 'content': ''})

        return sections

    async def _analyze_coverage(self, content: str, baseline: str) -> float:
        """åˆ†æè¦†ç›–åº¦ï¼šè®¡ç®—ä¸åŸºç¡€ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦"""
        # æå–åŸºç¡€ç‰ˆæœ¬çš„å…³é”®å…ƒç´ 
        baseline_sections = self._extract_sections(baseline)
        content_sections = self._extract_sections(content)

        if not baseline_sections:
            return 0.0

        # è®¡ç®—åŒ¹é…çš„ç« èŠ‚æ•°
        matched_sections = 0
        for baseline_section in baseline_sections:
            for content_section in content_sections:
                # ä½¿ç”¨å¤§æ¨¡å‹è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
                try:
                    similarity = await self._calculate_similarity(baseline_section['title'], content_section['title'])
                    if similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                        matched_sections += 1
                        break
                except Exception as e:
                    print(f"è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
                    # fallbackåˆ°ç®€å•åŒ¹é…
                    if baseline_section['title'].lower() in content_section['title'].lower():
                        matched_sections += 1
                        break

        return matched_sections / len(baseline_sections)


    async def _analyze_level_coverage(self, level_files: List[Dict[str, Any]], baseline_content: str, level: str) -> List[Dict[str, Any]]:
        """ä¸€æ¬¡æ€§åˆ†æä¸€ä¸ªlevelçš„æ‰€æœ‰æ–‡ä»¶è¦†ç›–åº¦"""
        # å‡†å¤‡æ‰€æœ‰æ–‡ä»¶å†…å®¹
        files_content = ""
        for i, file_data in enumerate(level_files, 1):
            files_content += f"\n=== {level}-{file_data['index']} ===\n"
            files_content += file_data['converted_content']
            files_content += "\n"

        # å¤§æ¨¡å‹åˆ†æè¦†ç›–åº¦
        prompt = f"""è¯·åˆ†æä»¥ä¸‹{len(level_files)}ä¸ª{level}çº§åˆ«ç”Ÿæˆçš„æµæ°´åˆ†æå¤§çº²ä¸åŸºç¡€ç‰ˆæœ¬çš„è¦†ç›–åº¦ã€‚

åŸºç¡€ç‰ˆæœ¬å¤§çº²ï¼š
{baseline_content}

ç”Ÿæˆçš„{level}å¤§çº²ï¼š
{files_content}

è¯·ä¸ºæ¯ä¸ªç”Ÿæˆçš„{level}å¤§çº²è®¡ç®—å…¶å¯¹åŸºç¡€ç‰ˆæœ¬çš„è¦†ç›–åº¦ï¼ˆ0-100%ï¼‰ï¼Œå¹¶ç»™å‡ºå¹³å‡è¦†ç›–åº¦ã€‚

è¾“å‡ºæ ¼å¼ï¼š
æ–‡ä»¶å: è¦†ç›–åº¦
æ–‡ä»¶å: è¦†ç›–åº¦
...
å¹³å‡è¦†ç›–åº¦: XX%"""

        try:
            messages = [
                ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œè¯·å‡†ç¡®è®¡ç®—è¦†ç›–åº¦ã€‚"),
                ("user", prompt)
            ]

            response = await self.llm.ainvoke(messages)
            content = response.content if hasattr(response, 'content') else str(response)

            # è§£æç»“æœ
            results = []
            lines = content.strip().split('\n')

            for file_data in level_files:
                filename = f"{level}-{file_data['index']}"
                coverage = 0.0

                # åœ¨å“åº”ä¸­æŸ¥æ‰¾å¯¹åº”çš„è¦†ç›–åº¦
                for line in lines:
                    if filename in line and ('%' in line or any(char.isdigit() for char in line)):
                        # æå–ç™¾åˆ†æ¯”æ•°å­—ï¼ŒåŒ¹é…æ–‡ä»¶ååé¢çš„æ•°å­—
                        match = re.search(rf'{filename}.*?(\d+(?:\.\d+)?)%?', line)
                        if match:
                            coverage = float(match.group(1))
                            break

                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾å•ç‹¬çš„è¡Œ
                if coverage == 0.0:
                    for line in lines:
                        if line.strip().startswith(filename) and ('%' in line or ':' in line):
                            # æå–å†’å·æˆ–ç™¾åˆ†å·åé¢çš„æ•°å­—
                            match = re.search(r'[:\s]+(\d+(?:\.\d+)?)%?', line)
                            if match:
                                coverage = float(match.group(1))
                                break

                # é™åˆ¶åœ¨0-100èŒƒå›´å†…
                coverage = max(0.0, min(100.0, coverage))

                analysis = {
                    "level": file_data['level'],
                    "index": file_data['index'],
                    "è¦†ç›–åº¦": f"{coverage:.1f}%"
                }
                results.append(analysis)
                print(f"      âœ“ {filename}: {coverage:.1f}%")

            return results

        except Exception as e:
            print(f"Levelè¦†ç›–åº¦åˆ†æå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤ç»“æœ
            return [{
                "level": file_data['level'],
                "index": file_data['index'],
                "è¦†ç›–åº¦": "0.0%"
            } for file_data in level_files]


    def _load_existing_files(self) -> List[Dict[str, Any]]:
        """åŠ è½½å·²æœ‰çš„mdæ–‡ä»¶ï¼ˆæ¯ä¸ªlevelå‰5ä¸ªï¼‰"""
        results = []
        print("  ğŸ“‚ å¼€å§‹åŠ è½½mdæ–‡ä»¶...")

        for level in ['level1', 'level2', 'level3']:
            print(f"    ğŸ” æ£€æŸ¥ {level} çº§åˆ«æ–‡ä»¶...")
            for i in range(1, 6):  # æ”¹ä¸º1-5
                filename = f"level{level[-1]}-{i}.md"
                filepath = self.test_dir / filename

                if filepath.exists():
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            content = f.read()

                        results.append({
                            "level": level,
                            "index": i,
                            "content": content,
                            "filepath": str(filepath)
                        })
                        print(f"      âœ“ åŠ è½½ {filename} ({len(content)} å­—ç¬¦)")
                    except Exception as e:
                        print(f"      âœ— åŠ è½½ {filename} å¤±è´¥: {str(e)}")
                else:
                    print(f"      âš  æ–‡ä»¶ä¸å­˜åœ¨: {filename}")

        print(f"  ğŸ“Š å…±åŠ è½½ {len(results)} ä¸ªæ–‡ä»¶")
        return results

    async def generate_report(self):
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        print("ğŸš€ å¼€å§‹ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š")
        print("=" * 60)
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")

        # ç¬¬ä¸€æ­¥ï¼šåŠ è½½å·²æœ‰æ–‡ä»¶
        print("\nğŸ“‚ ç¬¬ä¸€æ­¥ï¼šåŠ è½½å·²æœ‰mdæ–‡ä»¶")
        existing_files = self._load_existing_files()
        print(f"  âœ… ç¬¬ä¸€æ­¥å®Œæˆï¼Œå…±åŠ è½½ {len(existing_files)} ä¸ªæ–‡ä»¶")

        # ç¬¬äºŒæ­¥ï¼šæ ¼å¼è½¬æ¢
        print("\nğŸ”„ ç¬¬äºŒæ­¥ï¼šæ ¼å¼è½¬æ¢")
        converted_results = []
        for i, result in enumerate(existing_files):
            print(f"  ğŸ”„ è½¬æ¢ä¸­... ({i+1}/{len(existing_files)}) {result['level']}-{result['index']}")
            converted_content = self._convert_to_baseline_format(result['content'])
            result['converted_content'] = converted_content
            converted_results.append(result)
            print(f"    âœ“ level{result['level'][-1]}-{result['index']} æ ¼å¼è½¬æ¢å®Œæˆ")

        print(f"  âœ… ç¬¬äºŒæ­¥å®Œæˆï¼Œå…±è½¬æ¢ {len(converted_results)} ä¸ªæ–‡ä»¶")

        # ç¬¬ä¸‰æ­¥ï¼šæŒ‰levelåˆ†ç»„æ¯”è¾ƒåˆ†æ
        print("\nğŸ“Š ç¬¬ä¸‰æ­¥ï¼šæŒ‰levelåˆ†ç»„æ¯”è¾ƒåˆ†æï¼ˆæ¯æ¬¡ä¸€ä¸ªlevelï¼‰")
        analysis_results = []

        # æŒ‰levelåˆ†ç»„å¤„ç†
        for level in ['level1', 'level2', 'level3']:
            level_files = [f for f in converted_results if f['level'] == level]

            print(f"\n  ğŸ” å¤„ç† {level} çº§åˆ« ({len(level_files)} ä¸ªæ–‡ä»¶)")
            print(f"    ğŸ“‹ æ–‡ä»¶åˆ—è¡¨: {', '.join([f'{r['level']}-{r['index']}' for r in level_files])}")

            # ä¸€æ¬¡æ€§åˆ†ææ•´ä¸ªlevelçš„æ‰€æœ‰æ–‡ä»¶
            print(f"    ğŸ¤– è°ƒç”¨å¤§æ¨¡å‹åˆ†æ {level} çº§åˆ«è¦†ç›–åº¦...")
            level_analysis = await self._analyze_level_coverage(level_files, self.baseline_content, level)

            analysis_results.extend(level_analysis)
            print(f"    âœ… {level} çº§åˆ«åˆ†æå®Œæˆ")

        print(f"\n  âœ… ç¬¬ä¸‰æ­¥å®Œæˆï¼Œå…±åˆ†æ {len(analysis_results)} ä¸ªæ–‡ä»¶")

        # ç¬¬å››æ­¥ï¼šç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        print("\nğŸ“‹ ç¬¬å››æ­¥ï¼šç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š")
        self._generate_comparison_report(analysis_results)

        end_time = datetime.now()
        print(f"\nâœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼â° ç»“æŸæ—¶é—´: {end_time.strftime('%H:%M:%S')}")
        print("ğŸ“„ æ¯”è¾ƒæŠ¥å‘Šå·²ç”Ÿæˆï¼šæµæ°´æŠ¥å‘Šå¤§çº²å•å…ƒæµ‹è¯•/æ¯”è¾ƒæŠ¥å‘Š.md")

    def _generate_comparison_report(self, analysis_results: List[Dict[str, Any]]):
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        report_path = self.test_dir / "æ¯”è¾ƒæŠ¥å‘Š.md"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# æµæ°´åˆ†æå¤§çº²ç”Ÿæˆå•å…ƒæµ‹è¯•æ¯”è¾ƒæŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("**è¯´æ˜**: æœ¬æŠ¥å‘ŠåŸºäºå¤§æ¨¡å‹è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼Œä½¿ç”¨DeepSeekåˆ†ææ–‡æœ¬ç›¸ä¼¼æ€§\n\n")

            # æ€»ä½“ç»Ÿè®¡
            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")

            level_stats = {}
            for result in analysis_results:
                level = result['level']
                if level not in level_stats:
                    level_stats[level] = []

                # æå–æ•°å€¼
                coverage_pct = float(result['è¦†ç›–åº¦'].rstrip('%'))

                level_stats[level].append({
                    'coverage': coverage_pct
                })

            for level, stats in level_stats.items():
                coverages = [s['coverage'] for s in stats]
                avg_coverage = sum(coverages) / len(coverages)
                max_coverage = max(coverages)
                min_coverage = min(coverages)

                f.write(f"### {level.upper()} çº§åˆ«ç»Ÿè®¡\n")
                f.write(f"- æ ·æœ¬æ•°é‡: {len(stats)}\n")
                f.write(f"- å¹³å‡è¦†ç›–åº¦: {avg_coverage:.1f}%\n")
                f.write(f"- æœ€é«˜è¦†ç›–åº¦: {max_coverage:.1f}%\n")
                f.write(f"- æœ€ä½è¦†ç›–åº¦: {min_coverage:.1f}%\n\n")

            # è¯¦ç»†ç»“æœ
            f.write("## ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ\n\n")

            for level in ['level1', 'level2', 'level3']:
                f.write(f"### {level.upper()} çº§åˆ«ç»“æœ\n\n")
                f.write("| æ–‡ä»¶å | è¦†ç›–åº¦ |\n")
                f.write("|--------|--------|\n")

                level_results = [r for r in analysis_results if r['level'] == level]
                for result in level_results:
                    f.write(f"| level{level[-1]}-{result['index']} | {result['è¦†ç›–åº¦']} |\n")

                f.write("\n")

            # ç»“è®º
            f.write("## ğŸ¯ æµ‹è¯•ç»“è®º\n\n")

            # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®çš„è¡¨ç°
            all_coverages = [(r['level'], r['index'], float(r['è¦†ç›–åº¦'].rstrip('%'))) for r in analysis_results]

            best_coverage = max(all_coverages, key=lambda x: x[2])
            worst_coverage = min(all_coverages, key=lambda x: x[2])

            f.write("### æœ€ä½³è¡¨ç°\n")
            f.write(f"- æœ€é«˜è¦†ç›–åº¦: level{best_coverage[0][-1]}-{best_coverage[1]} ({best_coverage[2]:.1f}%)\n")
            f.write(f"- æœ€ä½è¦†ç›–åº¦: level{worst_coverage[0][-1]}-{worst_coverage[1]} ({worst_coverage[2]:.1f}%)\n\n")

            # çº§åˆ«å¯¹æ¯”
            f.write("### çº§åˆ«å¯¹æ¯”åˆ†æ\n")
            for level in ['level1', 'level2', 'level3']:
                level_data = [r for r in analysis_results if r['level'] == level]
                coverages = [float(r['è¦†ç›–åº¦'].rstrip('%')) for r in level_data]
                avg_cov = sum(coverages) / len(coverages)
                max_cov = max(coverages)
                min_cov = min(coverages)

                f.write(f"- **{level.upper()}**: å¹³å‡è¦†ç›–åº¦ {avg_cov:.1f}%, èŒƒå›´ {min_cov:.1f}%-{max_cov:.1f}%\n")

            f.write("\n### ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•è¯´æ˜\n")
            f.write("- **è¦†ç›–åº¦**: ä½¿ç”¨DeepSeekå¤§æ¨¡å‹è®¡ç®—ç”Ÿæˆå†…å®¹ä¸åŸºç¡€ç‰ˆæœ¬çš„ç« èŠ‚æ ‡é¢˜è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ŒåŒ¹é…åº¦è¶…è¿‡60%çš„ç« èŠ‚è®¡ä¸ºè¦†ç›–\n")
            f.write("- **ä¸“ä¸šåº¦**: åŸºäºä¸“ä¸šæœ¯è¯­ä½¿ç”¨ã€ç»“æ„å®Œæ•´æ€§ã€å†…å®¹ä¸°å¯Œåº¦ç­‰ç»´åº¦è¿›è¡Œç»¼åˆè¯„åˆ†\n")
            f.write("- **å…¶ä»–æŒ‡æ ‡**: åŒ…æ‹¬ç»“æ„å®Œæ•´æ€§è¯„ä¼°ã€å†…å®¹ä¸°å¯Œåº¦åˆ†æã€ä¸“ä¸šæœ¯è¯­ç»Ÿè®¡ç­‰\n\n")

            f.write("### ğŸ“Š è¦†ç›–åº¦ç»Ÿè®¡æ€»ç»“\n")
            all_coverages = [float(r['è¦†ç›–åº¦'].rstrip('%')) for r in analysis_results]
            overall_avg = sum(all_coverages) / len(all_coverages)
            overall_max = max(all_coverages)
            overall_min = min(all_coverages)

            f.write(f"- æ€»æ ·æœ¬æ•°: {len(analysis_results)}\n")
            f.write(f"- æ•´ä½“å¹³å‡è¦†ç›–åº¦: {overall_avg:.1f}%\n")
            f.write(f"- æœ€é«˜è¦†ç›–åº¦: {overall_max:.1f}%\n")
            f.write(f"- æœ€ä½è¦†ç›–åº¦: {overall_min:.1f}%\n\n")

            f.write("### æŠ€æœ¯è¯´æ˜\n")
            f.write("- **è¦†ç›–åº¦è®¡ç®—**: ä½¿ç”¨DeepSeekå¤§æ¨¡å‹ä¸€æ¬¡æ€§åˆ†ææ¯ä¸ªlevelçš„5ä¸ªæ–‡ä»¶ä¸åŸºç¡€ç‰ˆæœ¬çš„è¦†ç›–åº¦\n")
            f.write("- **æŒ‰levelåˆ†ç»„**: æ¯æ¬¡åªè°ƒç”¨ä¸€æ¬¡å¤§æ¨¡å‹ï¼Œåˆ†æä¸€ä¸ªlevelçš„æ‰€æœ‰æ–‡ä»¶\n")
            f.write("- **é«˜æ•ˆå¤„ç†**: æ€»å…±åªè°ƒç”¨3æ¬¡å¤§æ¨¡å‹ï¼Œå¿«é€Ÿå®Œæˆåˆ†æ\n")

        print(f"ğŸ“„ æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


async def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è·å–APIé…ç½®
    import os
    from dotenv import load_dotenv

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    DEEPSEEK_API_KEY = "sk-438668d443224063adbb1d295fe44a9f"
    DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

    # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
    generator = ComparisonReportGenerator(DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL)

    # ç”ŸæˆæŠ¥å‘Š
    await generator.generate_report()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
