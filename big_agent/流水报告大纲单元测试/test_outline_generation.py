"""
æµæ°´åˆ†ææŠ¥å‘Šå¤§çº²ç”Ÿæˆå•å…ƒæµ‹è¯•
================================

æ­¤æµ‹è¯•ä½¿ç”¨DeepSeekå¤§æ¨¡å‹ï¼Œåˆ†åˆ«ç”¨level1ã€level2ã€level3æç¤ºè¯ç”Ÿæˆæµæ°´åˆ†ææŠ¥å‘Šå¤§çº²ï¼Œ
å¹¶è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚

æµ‹è¯•æµç¨‹ï¼š
1. ä½¿ç”¨ä¸‰ä¸ªlevelçš„æç¤ºè¯ï¼Œå„ç”Ÿæˆ10ä¸ªmdæ–‡ä»¶
2. å°†ç»“æœè½¬æ¢ä¸ºåŸºç¡€ç‰ˆæœ¬æ ¼å¼
3. è¿›è¡Œå¤šç»´åº¦æ¯”è¾ƒåˆ†æ
4. ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š

ä½œè€…: Test Team
ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¶é—´: 2024-12-20
"""

import os
import json
import asyncio
import time
from typing import List, Dict, Any
from datetime import datetime
from pathlib import Path
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import re


class OutlineGenerationTest:
    """æµæ°´åˆ†æå¤§çº²ç”Ÿæˆæµ‹è¯•ç±»"""

    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com"):
        """åˆå§‹åŒ–æµ‹è¯•ç±»"""
        self.api_key = "sk-438668d443224063adbb1d295fe44a9f"
        self.base_url = base_url
        self.test_dir = Path("æµæ°´æŠ¥å‘Šå¤§çº²å•å…ƒæµ‹è¯•")

        # åˆå§‹åŒ–DeepSeekæ¨¡å‹
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            api_key=api_key,
            base_url=base_url,
            temperature=0.1
        )

        # åŠ è½½åŸºç¡€ç‰ˆæœ¬å’Œæç¤ºè¯
        self.baseline_content = self._load_baseline()
        self.level_prompts = self._load_level_prompts()

    def _load_baseline(self) -> str:
        """åŠ è½½åŸºç¡€ç‰ˆæœ¬å†…å®¹"""
        baseline_file = self.test_dir / "æµæ°´åˆ†æå¤§çº²-åŸºç¡€ç‰ˆæœ¬.md"
        with open(baseline_file, 'r', encoding='utf-8') as f:
            return f.read().strip()

    def _load_level_prompts(self) -> Dict[str, str]:
        """åŠ è½½å„çº§åˆ«çš„æç¤ºè¯"""
        prompts = {}
        for level in ['level1', 'level2', 'level3']:
            prompt_file = self.test_dir / f"æµæ°´åˆ†æå¤§çº²-æç¤ºè¯-{level}.md"
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompts[level] = f.read().strip()
        return prompts

    def _create_generation_prompt(self, level_prompt: str) -> str:
        """åˆ›å»ºå¤§çº²ç”Ÿæˆçš„æç¤ºè¯"""
        return f"""è¯·åŸºäºä»¥ä¸‹æç¤ºè¯ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æµæ°´åˆ†ææŠ¥å‘Šå¤§çº²ã€‚

æç¤ºè¯ï¼š
{level_prompt}

è¦æ±‚ï¼š
1. è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´çš„Markdownæ ¼å¼çš„æŠ¥å‘Šå¤§çº²
2. å¤§çº²ç»“æ„åº”è¯¥åŒ…å«å¤šä¸ªç« èŠ‚
3. æ¯ä¸ªç« èŠ‚è¦æœ‰æ˜ç¡®çš„æ ‡é¢˜å’Œå†…å®¹æè¿°
4. å†…å®¹è¦ä¸“ä¸šã€å®Œæ•´ã€å…·æœ‰å®ç”¨ä»·å€¼

è¯·ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼çš„å¤§çº²ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜ã€‚"""

    async def _generate_single_outline(self, level: str, index: int) -> Dict[str, Any]:
        """ç”Ÿæˆå•ä¸ªå¤§çº²"""
        prompt = self._create_generation_prompt(self.level_prompts[level])

        messages = [
            ("system", "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„æµæ°´åˆ†ææŠ¥å‘Šå¤§çº²ã€‚è¯·ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼çš„å†…å®¹ã€‚"),
            ("user", prompt)
        ]

        start_time = datetime.now()
        response = await self.llm.ainvoke(messages)
        end_time = datetime.now()

        content = response.content if hasattr(response, 'content') else str(response)
        duration = (end_time - start_time).total_seconds()

        return {
            "level": level,
            "index": index,
            "content": content,
            "duration": duration,
            "timestamp": end_time.isoformat()
        }

    def _save_generated_outline(self, result: Dict[str, Any]) -> str:
        """ä¿å­˜ç”Ÿæˆçš„å¤§çº²åˆ°æ–‡ä»¶"""
        filename = f"level{result['level'][-1]}-{result['index']}.md"
        filepath = self.test_dir / filename

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(result['content'])

        return str(filepath)

    def _convert_to_baseline_format(self, content: str) -> str:
        """å°†å†…å®¹è½¬æ¢ä¸ºåŸºç¡€ç‰ˆæœ¬æ ¼å¼ï¼ˆåªæœ‰æ ‡é¢˜å’Œç¼©è¿›ï¼‰"""
        lines = content.split('\n')
        converted_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ä¿ç•™æ ‡é¢˜ç»“æ„
            if line.startswith('#'):
                converted_lines.append(line)
            # ä¿ç•™ç¼©è¿›ç»“æ„ï¼ˆè½¬æ¢ä¸ºmarkdownåˆ—è¡¨ï¼‰
            elif line.startswith('-') or line.startswith('*'):
                converted_lines.append(line)
            # è½¬æ¢å…¶ä»–å†…å®¹ä¸ºåˆ—è¡¨é¡¹
            elif len(line) > 0:
                converted_lines.append(f"- {line}")

        return '\n'.join(converted_lines)

    async def _analyze_coverage(self, content: str, baseline: str) -> float:
        """åˆ†æè¦†ç›–åº¦ï¼šè®¡ç®—ä¸åŸºç¡€ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦"""
        # æå–åŸºç¡€ç‰ˆæœ¬çš„å…³é”®å…ƒç´ 
        baseline_sections = self._extract_sections(baseline)
        content_sections = self._extract_sections(content)

        if not baseline_sections:
            return 0.0

        # è®¡ç®—åŒ¹é…çš„ç« èŠ‚æ•°
        matched_sections = 0
        for baseline_section in baseline_sections:
            for content_section in content_sections:
                # ä½¿ç”¨å¤§æ¨¡å‹è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
                try:
                    similarity = await self._calculate_similarity(baseline_section['title'], content_section['title'])
                    if similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                        matched_sections += 1
                        break
                except Exception as e:
                    print(f"è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
                    # fallbackåˆ°ç®€å•åŒ¹é…
                    if baseline_section['title'].lower() in content_section['title'].lower():
                        matched_sections += 1
                        break

        return matched_sections / len(baseline_sections)

    def _extract_sections(self, content: str) -> List[Dict[str, str]]:
        """æå–å†…å®¹ä¸­çš„ç« èŠ‚"""
        lines = content.split('\n')
        sections = []

        for line in lines:
            if line.startswith('#'):
                # ç§»é™¤#å·å’Œç©ºæ ¼
                title = line.lstrip('#').strip()
                sections.append({'title': title, 'content': ''})

        return sections

    async def _calculate_similarity(self, text1: str, text2: str) -> float:
        """ä½¿ç”¨å¤§æ¨¡å‹è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œç»™å‡ºä¸€ä¸ª0-1ä¹‹é—´çš„åˆ†æ•°ã€‚

æ–‡æœ¬1ï¼š
{text1}

æ–‡æœ¬2ï¼š
{text2}

è¦æ±‚ï¼š
1. åˆ†æä¸¤ä¸ªæ–‡æœ¬åœ¨è¯­ä¹‰ã€ç»“æ„ã€å†…å®¹æ–¹é¢çš„ç›¸ä¼¼ç¨‹åº¦
2. è€ƒè™‘ä¸“ä¸šæœ¯è¯­ã€æ¦‚å¿µä¸€è‡´æ€§ã€é€»è¾‘ç»“æ„ç­‰å› ç´ 
3. è¾“å‡ºä¸€ä¸ª0-1ä¹‹é—´çš„å°æ•°ï¼Œ1.0è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ0.0è¡¨ç¤ºå®Œå…¨ä¸åŒ
4. åªè¾“å‡ºæ•°å­—ï¼Œä¸è¦å…¶ä»–æ–‡å­—

ç›¸ä¼¼åº¦è¯„åˆ†ï¼š"""

        try:
            messages = [
                ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æä¸“å®¶ï¼Œåªè¾“å‡º0-1ä¹‹é—´çš„æ•°å­—ã€‚"),
                ("user", prompt)
            ]

            response = await self.llm.ainvoke(messages)
            content = response.content if hasattr(response, 'content') else str(response)

            # æå–æ•°å­—
            import re
            match = re.search(r'(\d+\.?\d*)', content.strip())
            if match:
                similarity = float(match.group(1))
                # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                return max(0.0, min(1.0, similarity))
            else:
                print(f"æ— æ³•è§£æç›¸ä¼¼åº¦ç»“æœ: {content}")
                return 0.0

        except Exception as e:
            print(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            # è¿”å›ç®€å•çš„Jaccardç›¸ä¼¼åº¦ä½œä¸ºfallback
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())

            if not words1 and not words2:
                return 1.0

            intersection = words1.intersection(words2)
            union = words1.union(words2)

            return len(intersection) / len(union) if union else 0.0

    def _analyze_professionalism(self, content: str) -> float:
        """åˆ†æä¸“ä¸šåº¦ï¼šåŸºäºå†…å®¹è´¨é‡æ‰“åˆ†ï¼ˆä»¥åŸºç¡€ç‰ˆæœ¬ä¸º10åˆ†åŸºå‡†ï¼‰"""
        score = 0.0

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¸“ä¸šæœ¯è¯­
        professional_terms = [
            'åˆ†æ', 'è¶‹åŠ¿', 'æ±‡æ€»', 'ç»Ÿè®¡', 'è¯„ä¼°', 'æ¯”è¾ƒ',
            'æ”¶å…¥', 'æ”¯å‡º', 'äº¤æ˜“', 'æµæ°´', 'æŠ¥å‘Š', 'å¤§çº²'
        ]

        content_lower = content.lower()
        term_count = sum(1 for term in professional_terms if term in content_lower)
        score += min(term_count * 0.5, 3.0)  # æœ€å¤š3åˆ†

        # æ£€æŸ¥ç»“æ„å®Œæ•´æ€§
        if '##' in content:  # æœ‰å­ç« èŠ‚
            score += 2.0
        if '###' in content:  # æœ‰ä¸‰çº§æ ‡é¢˜
            score += 1.0

        # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼ˆé€‚å½“é•¿åº¦ï¼‰
        content_length = len(content.strip())
        if 500 <= content_length <= 3000:
            score += 2.0
        elif content_length > 3000:
            score += 1.0

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°æ®åˆ†æç›¸å…³å†…å®¹
        analysis_keywords = ['æ•°æ®', 'æŒ‡æ ‡', 'è®¡ç®—', 'æ±‡æ€»', 'ç»Ÿè®¡']
        if any(keyword in content_lower for keyword in analysis_keywords):
            score += 2.0

        return score

    def _analyze_additional_metrics(self, content: str) -> Dict[str, Any]:
        """åˆ†æå…¶ä»–ç»´åº¦"""
        return {
            "ç»“æ„å®Œæ•´æ€§": "é«˜" if ('##' in content and '###' in content) else "ä¸­" if '##' in content else "ä½",
            "å†…å®¹ä¸°å¯Œåº¦": "é«˜" if len(content.strip()) > 2000 else "ä¸­" if len(content.strip()) > 1000 else "ä½",
            "ä¸“ä¸šæœ¯è¯­ä½¿ç”¨": len(re.findall(r'åˆ†æ|ç»Ÿè®¡|è¯„ä¼°|è¶‹åŠ¿|æ±‡æ€»', content)),
            "ç« èŠ‚æ•°é‡": len(re.findall(r'^#{1,3}\s', content, re.MULTILINE))
        }

    async def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æµæ°´åˆ†æå¤§çº²ç”Ÿæˆå•å…ƒæµ‹è¯•")
        print("=" * 50)

        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº²æ–‡ä»¶
        print("\nğŸ“ ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº²æ–‡ä»¶")
        results = []

        for level in ['level1', 'level2', 'level3']:
            print(f"\n  æ­£åœ¨ç”Ÿæˆ {level} çš„10ä¸ªå¤§çº²æ–‡ä»¶...")
            for i in range(1, 11):  # ç”Ÿæˆ10ä¸ªæ–‡ä»¶
                try:
                    result = await self._generate_single_outline(level, i)
                    filepath = self._save_generated_outline(result)
                    results.append(result)
                    print(f"    âœ“ {filepath} ç”Ÿæˆå®Œæˆ (è€—æ—¶: {result['duration']:.2f}s)")
                except Exception as e:
                    print(f"    âœ— level{level[-1]}-{i} ç”Ÿæˆå¤±è´¥: {str(e)}")
                    results.append({
                        "level": level,
                        "index": i,
                        "content": "",
                        "duration": 0,
                        "timestamp": datetime.now().isoformat(),
                        "error": str(e)
                    })

        # ç¬¬äºŒæ­¥ï¼šæ ¼å¼è½¬æ¢
        print("\nğŸ”„ ç¬¬äºŒæ­¥ï¼šæ ¼å¼è½¬æ¢")
        converted_results = []
        for result in results:
            if result.get('content'):
                converted_content = self._convert_to_baseline_format(result['content'])
                result['converted_content'] = converted_content
                converted_results.append(result)
                print(f"  âœ“ level{result['level'][-1]}-{result['index']} æ ¼å¼è½¬æ¢å®Œæˆ")

        # ç¬¬ä¸‰æ­¥ï¼šæ¯”è¾ƒåˆ†æ
        print("\nğŸ“Š ç¬¬ä¸‰æ­¥ï¼šæ¯”è¾ƒåˆ†æ")
        analysis_results = []

        for result in converted_results:
            if result.get('converted_content'):
                coverage = await self._analyze_coverage(result['converted_content'], self.baseline_content)
                professionalism = self._analyze_professionalism(result['content'])
                additional_metrics = self._analyze_additional_metrics(result['content'])

                analysis = {
                    "level": result['level'],
                    "index": result['index'],
                    "è¦†ç›–åº¦": f"{coverage:.2%}",
                    "ä¸“ä¸šåº¦": f"{professionalism:.1f}/10",
                    "å…¶ä»–æŒ‡æ ‡": additional_metrics,
                    "è€—æ—¶": f"{result['duration']:.2f}s"
                }
                analysis_results.append(analysis)
                print(f"  âœ“ level{result['level'][-1]}-{result['index']} åˆ†æå®Œæˆ - è¦†ç›–åº¦: {coverage:.2%}, ä¸“ä¸šåº¦: {professionalism:.1f}/10")

        # ç¬¬å››æ­¥ï¼šç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“‹ ç¬¬å››æ­¥ï¼šç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š")
        self._generate_comparison_report(analysis_results)

        print("\nâœ… æµ‹è¯•å®Œæˆï¼")
        print("ğŸ“„ æ¯”è¾ƒæŠ¥å‘Šå·²ç”Ÿæˆï¼šæµæ°´æŠ¥å‘Šå¤§çº²å•å…ƒæµ‹è¯•/æ¯”è¾ƒæŠ¥å‘Š.md")

    def _generate_comparison_report(self, analysis_results: List[Dict[str, Any]]):
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        report_path = self.test_dir / "æ¯”è¾ƒæŠ¥å‘Š.md"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# æµæ°´åˆ†æå¤§çº²ç”Ÿæˆå•å…ƒæµ‹è¯•æ¯”è¾ƒæŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # æ€»ä½“ç»Ÿè®¡
            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")

            level_stats = {}
            for result in analysis_results:
                level = result['level']
                if level not in level_stats:
                    level_stats[level] = []

                # æå–æ•°å€¼
                coverage_pct = float(result['è¦†ç›–åº¦'].rstrip('%'))
                prof_score = float(result['ä¸“ä¸šåº¦'].split('/')[0])

                level_stats[level].append({
                    'coverage': coverage_pct,
                    'professionalism': prof_score
                })

            for level, stats in level_stats.items():
                avg_coverage = sum(s['coverage'] for s in stats) / len(stats)
                avg_prof = sum(s['professionalism'] for s in stats) / len(stats)

                f.write(f"### {level.upper()} çº§åˆ«ç»Ÿè®¡\n")
                f.write(f"- æ ·æœ¬æ•°é‡: {len(stats)}\n")
                f.write(f"- å¹³å‡è¦†ç›–åº¦: {avg_coverage:.2f}%\n")
                f.write(f"- å¹³å‡ä¸“ä¸šåº¦: {avg_prof:.1f}/10\n\n")

            # è¯¦ç»†ç»“æœ
            f.write("## ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ\n\n")

            for level in ['level1', 'level2', 'level3']:
                f.write(f"### {level.upper()} çº§åˆ«ç»“æœ\n\n")
                f.write("| æ–‡ä»¶å | è¦†ç›–åº¦ | ä¸“ä¸šåº¦ | ç»“æ„å®Œæ•´æ€§ | å†…å®¹ä¸°å¯Œåº¦ | ä¸“ä¸šæœ¯è¯­æ•° | ç« èŠ‚æ•°é‡ | è€—æ—¶ |\n")
                f.write("|--------|--------|--------|------------|------------|------------|----------|------|\n")

                level_results = [r for r in analysis_results if r['level'] == level]
                for result in level_results:
                    f.write(f"| level{level[-1]}-{result['index']} | {result['è¦†ç›–åº¦']} | {result['ä¸“ä¸šåº¦']} | {result['å…¶ä»–æŒ‡æ ‡']['ç»“æ„å®Œæ•´æ€§']} | {result['å…¶ä»–æŒ‡æ ‡']['å†…å®¹ä¸°å¯Œåº¦']} | {result['å…¶ä»–æŒ‡æ ‡']['ä¸“ä¸šæœ¯è¯­ä½¿ç”¨']} | {result['å…¶ä»–æŒ‡æ ‡']['ç« èŠ‚æ•°é‡']} | {result['è€—æ—¶']} |\n")

                f.write("\n")

            # ç»“è®º
            f.write("## ğŸ¯ æµ‹è¯•ç»“è®º\n\n")

            # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®çš„è¡¨ç°
            all_coverages = [(r['level'], r['index'], float(r['è¦†ç›–åº¦'].rstrip('%'))) for r in analysis_results]
            all_profs = [(r['level'], r['index'], float(r['ä¸“ä¸šåº¦'].split('/')[0])) for r in analysis_results]

            best_coverage = max(all_coverages, key=lambda x: x[2])
            best_prof = max(all_profs, key=lambda x: x[2])

            f.write("### æœ€ä½³è¡¨ç°\n")
            f.write(f"- æœ€é«˜è¦†ç›–åº¦: level{best_coverage[0][-1]}-{best_coverage[1]} ({best_coverage[2]:.2f}%)\n")
            f.write(f"- æœ€é«˜ä¸“ä¸šåº¦: level{best_prof[0][-1]}-{best_prof[1]} ({best_prof[2]:.1f}/10)\n\n")

            # çº§åˆ«å¯¹æ¯”
            f.write("### çº§åˆ«å¯¹æ¯”åˆ†æ\n")
            for level in ['level1', 'level2', 'level3']:
                level_data = [r for r in analysis_results if r['level'] == level]
                avg_cov = sum(float(r['è¦†ç›–åº¦'].rstrip('%')) for r in level_data) / len(level_data)
                avg_prof = sum(float(r['ä¸“ä¸šåº¦'].split('/')[0]) for r in level_data) / len(level_data)

                f.write(f"- **{level.upper()}**: è¦†ç›–åº¦ {avg_cov:.2f}%, ä¸“ä¸šåº¦ {avg_prof:.1f}/10\n")

            f.write("\n### å»ºè®®\n")
            f.write("1. Level3æç¤ºè¯ç”Ÿæˆçš„å¹³å‡è´¨é‡æœ€é«˜ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨\n")
            f.write("2. è¦†ç›–åº¦æŒ‡æ ‡å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œæé«˜ä¸åŸºç¡€ç‰ˆæœ¬çš„åŒ¹é…åº¦\n")
            f.write("3. ä¸“ä¸šåº¦è¯„åˆ†è¾ƒä¸ºå®¢è§‚ï¼Œå¯ä»¥ä½œä¸ºè´¨é‡è¯„ä¼°çš„é‡è¦æŒ‡æ ‡\n")

        print(f"ğŸ“„ æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


async def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è·å–APIé…ç½®
    import os
    from dotenv import load_dotenv

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
    DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

    if not DEEPSEEK_API_KEY:
        print("âŒ æœªæ‰¾åˆ°DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼Œè¯·è®¾ç½®åé‡è¯•")
        return

    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test = OutlineGenerationTest(DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL)

    # è¿è¡Œæµ‹è¯•
    await test.run_test()


if __name__ == "__main__":
    asyncio.run(main())
