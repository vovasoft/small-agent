#!/usr/bin/env python3
"""
è§„åˆ™ç”Ÿæˆå™¨æµ‹è¯•è„šæœ¬
=================

æ”¯æŒä»CSVæˆ–Excelæ–‡ä»¶æ‰¹é‡å¯¼å…¥æŒ‡æ ‡å®šä¹‰è¿›è¡Œæµ‹è¯•ã€‚
ä¼šä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆè§„åˆ™å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶ã€‚

æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š
- CSVæ–‡ä»¶ï¼šæŒ‡æ ‡åç§°,æŒ‡æ ‡æè¿°
- Excelæ–‡ä»¶ï¼šxlsxæ ¼å¼ï¼Œç¬¬ä¸€åˆ—æŒ‡æ ‡åç§°ï¼Œç¬¬äºŒåˆ—æŒ‡æ ‡æè¿°

ä½œè€…: Big Agent Team
ç‰ˆæœ¬: 2.0.0
"""

import json
import os
import sys
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL

# å¯¼å…¥è§„åˆ™ç”Ÿæˆå™¨
from rule_generator.generator import RuleGenerator


def load_metrics_from_file(file_path: str) -> pd.DataFrame:
    """
    ä»CSVæˆ–Excelæ–‡ä»¶åŠ è½½æŒ‡æ ‡å®šä¹‰

    Args:
        file_path: æ–‡ä»¶è·¯å¾„

    Returns:
        åŒ…å«æŒ‡æ ‡åç§°å’Œæè¿°çš„DataFrame
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    file_extension = Path(file_path).suffix.lower()

    try:
        if file_extension == '.csv':
            df = pd.read_csv(file_path, encoding='utf-8')
        elif file_extension in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚è¯·ä½¿ç”¨CSVæˆ–Excelæ–‡ä»¶ã€‚")

        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_columns = ['æŒ‡æ ‡åç§°', 'æŒ‡æ ‡æè¿°']
        if not all(col in df.columns for col in required_columns):
            raise ValueError(f"æ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—: {required_columns}")

        # æ¸…ç†æ•°æ®ï¼šåˆ é™¤ç©ºè¡Œ
        df = df.dropna(subset=required_columns)

        print(f"âœ“ æˆåŠŸåŠ è½½ {len(df)} ä¸ªæŒ‡æ ‡å®šä¹‰")
        return df

    except Exception as e:
        raise Exception(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")


def generate_rule_id(metric_name: str) -> str:
    """
    æ ¹æ®æŒ‡æ ‡åç§°ç”Ÿæˆè§„åˆ™ID

    Args:
        metric_name: æŒ‡æ ‡åç§°

    Returns:
        è§„åˆ™IDå­—ç¬¦ä¸²
    """
    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼Œè½¬æ¢ä¸ºè‹±æ–‡æ ‡è¯†ç¬¦æ ¼å¼
    import re
    clean_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', metric_name)
    clean_name = re.sub(r'_+', '_', clean_name).strip('_')

    # ç”ŸæˆIDï¼Œé™åˆ¶é•¿åº¦
    rule_id = f"metric-{clean_name}"
    if len(rule_id) > 50:
        rule_id = rule_id[:47] + "..."

    return rule_id


def generate_single_metric(generator, metric_data, output_dir):
    """
    ç”Ÿæˆå•ä¸ªæŒ‡æ ‡çš„è§„åˆ™

    Args:
        generator: RuleGeneratorå®ä¾‹
        metric_data: åŒ…å«æŒ‡æ ‡ä¿¡æ¯çš„å­—å…¸
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        ç”Ÿæˆç»“æœå­—å…¸
    """
    idx, row = metric_data['idx'], metric_data['row']
    metric_name = str(row['æŒ‡æ ‡åç§°']).strip()
    metric_desc = str(row['æŒ‡æ ‡æè¿°']).strip()

    try:
        start_time = time.time()

        # ç”Ÿæˆè§„åˆ™ID
        rule_id = generate_rule_id(metric_name)

        # åˆ›å»ºpayload
        payload = generator.create_decision_knowledge_payload(
            id=rule_id,
            name=metric_name,
            ruleDescription=metric_desc
        )

        # ç”Ÿæˆæ–‡ä»¶å
        safe_filename = re.sub(r'[^\w\u4e00-\u9fff]', '_', metric_name)
        filename = f"{output_dir}/rule_{safe_filename}.json"

        # ä¿å­˜JSONæ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)

        elapsed_time = time.time() - start_time

        return {
            'success': True,
            'metric_name': metric_name,
            'filename': filename,
            'rule_steps': len(payload['ruleDefinition']['ruleContent']),
            'elapsed_time': elapsed_time,
            'error': None
        }

    except Exception as e:
        return {
            'success': False,
            'metric_name': metric_name,
            'filename': None,
            'rule_steps': 0,
            'elapsed_time': 0,
            'error': str(e)
        }


def generate_metrics_batch(file_path: str, max_workers: int = 3, skip_api_save: bool = True):
    """
    æ‰¹é‡ç”ŸæˆæŒ‡æ ‡è§„åˆ™

    Args:
        file_path: åŒ…å«æŒ‡æ ‡å®šä¹‰çš„æ–‡ä»¶è·¯å¾„
        max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼Œé¿å…APIé™æµï¼‰
        skip_api_save: æ˜¯å¦è·³è¿‡APIä¿å­˜ï¼ˆé»˜è®¤Trueï¼Œåªç”ŸæˆJSONæ–‡ä»¶ï¼‰
    """
    print("è§„åˆ™ç”Ÿæˆå™¨ - æ‰¹é‡æŒ‡æ ‡è§„åˆ™ç”Ÿæˆ")
    print("============================")

    # æ£€æŸ¥ç¯å¢ƒé…ç½®
    if not DEEPSEEK_API_KEY:
        print("âœ— DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return False

    print("âœ“ APIé…ç½®æ­£å¸¸")
    print(f"âš™ï¸  å¹¶å‘æ•°: {max_workers}")
    print(f"ğŸ’¾ è·³è¿‡APIä¿å­˜: {skip_api_save}")

    try:
        # åŠ è½½æŒ‡æ ‡å®šä¹‰
        df = load_metrics_from_file(file_path)
        total_metrics = len(df)

        # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼ˆæ¯ä¸ªçº¿ç¨‹éƒ½éœ€è¦è‡ªå·±çš„å®ä¾‹ï¼‰
        def create_generator():
            return RuleGenerator(
                api_key=DEEPSEEK_API_KEY,
                base_url=DEEPSEEK_BASE_URL
            )

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "generated_metrics_rules"
        os.makedirs(output_dir, exist_ok=True)

        print(f"\nğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {total_metrics} ä¸ªæŒ‡æ ‡...")

        # å‡†å¤‡ä»»åŠ¡æ•°æ®
        tasks = [{'idx': idx, 'row': row} for idx, row in df.iterrows()]

        success_count = 0
        generated_files = []
        total_time = 0

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(generate_single_metric, create_generator(), task, output_dir): task
                for task in tasks
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                idx = task['idx'] + 1

                try:
                    result = future.result()

                    if result['success']:
                        print(f"âœ“ [{idx:2d}/{total_metrics}] {result['metric_name'][:20]:20} "
                              f"({result['elapsed_time']:.1f}s, {result['rule_steps']}æ­¥) -> {os.path.basename(result['filename'])}")
                        success_count += 1
                        generated_files.append(result['filename'])
                        total_time += result['elapsed_time']
                    else:
                        print(f"âœ— [{idx:2d}/{total_metrics}] {result['metric_name'][:20]:20} -> {result['error']}")

                except Exception as e:
                    metric_name = str(task['row']['æŒ‡æ ‡åç§°']).strip()
                    print(f"âœ— [{idx:2d}/{total_metrics}] {metric_name[:20]:20} -> ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = total_time / success_count if success_count > 0 else 0

        print(f"\nğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸç”Ÿæˆ: {success_count}/{total_metrics} ä¸ªæŒ‡æ ‡è§„åˆ™")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"ğŸ“ˆ å¹³å‡è€—æ—¶: {avg_time:.1f}ç§’/æŒ‡æ ‡")
        print(f"âš¡ å¹¶å‘æ•ˆç‡: {total_time/max_workers:.1f}ç§’ (ç†è®ºæœ€å°æ—¶é—´)")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        if generated_files:
            print("ğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
            for file in generated_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {os.path.basename(file)}")
            if len(generated_files) > 5:
                print(f"   ... è¿˜æœ‰ {len(generated_files) - 5} ä¸ªæ–‡ä»¶")

        return True

    except Exception as e:
        print(f"âœ— æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
        return False


def generate_and_save_json():
    """ç”Ÿæˆå¹¶ä¿å­˜JSONæ–‡ä»¶"""
    print("è§„åˆ™ç”Ÿæˆå™¨ - JSONæ–‡ä»¶ç”Ÿæˆ")
    print("========================")

    # æ£€æŸ¥ç¯å¢ƒé…ç½®
    if not DEEPSEEK_API_KEY:
        print("âœ— DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return

    print("âœ“ APIé…ç½®æ­£å¸¸")

    try:
        generator = RuleGenerator(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL
        )

        # æµ‹è¯•ç”¨ä¾‹1ï¼šç®€å•ç­›é€‰è§„åˆ™
        print("\nç”Ÿæˆæµ‹è¯•ç”¨ä¾‹1ï¼šé‡‘é¢ç­›é€‰è§„åˆ™")
        payload1 = generator.create_decision_knowledge_payload(
            id="filter-amount-001",
            name="é‡‘é¢ç­›é€‰è§„åˆ™",
            ruleDescription="ç­›é€‰å‡ºäº¤æ˜“é‡‘é¢å¤§äº1000çš„è®°å½•"
        )

        # ä¿å­˜JSONæ–‡ä»¶
        filename1 = "generated_rule_filter_amount.json"
        with open(filename1, 'w', encoding='utf-8') as f:
            json.dump(payload1, f, ensure_ascii=False, indent=2)

        print(f"âœ“ JSONæ–‡ä»¶å·²ä¿å­˜: {filename1}")
        print(f"âœ“ ç”Ÿæˆäº† {len(payload1['ruleDefinition']['ruleContent'])} ä¸ªè§„åˆ™æ­¥éª¤")

        # æ˜¾ç¤ºè§„åˆ™å†…å®¹æ‘˜è¦
        for i, step in enumerate(payload1['ruleDefinition']['ruleContent'], 1):
            print(f"  æ­¥éª¤{i}: {step.get('type', 'UNKNOWN')}")

        # æµ‹è¯•ç”¨ä¾‹2ï¼šåˆ†ç»„èšåˆè§„åˆ™
        print("\nç”Ÿæˆæµ‹è¯•ç”¨ä¾‹2ï¼šåˆ†ç»„èšåˆè§„åˆ™")
        payload2 = generator.create_decision_knowledge_payload(
            id="group-aggregate-002",
            name="åˆ†ç»„èšåˆè§„åˆ™",
            ruleDescription="æŒ‰äº¤æ˜“å¯¹æ‰‹åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªå¯¹æ‰‹çš„æ€»äº¤æ˜“é‡‘é¢ï¼Œé™åºæ’åˆ—å–å‰3å"
        )

        # ä¿å­˜JSONæ–‡ä»¶
        filename2 = "generated_rule_group_aggregate.json"
        with open(filename2, 'w', encoding='utf-8') as f:
            json.dump(payload2, f, ensure_ascii=False, indent=2)

        print(f"âœ“ JSONæ–‡ä»¶å·²ä¿å­˜: {filename2}")
        print(f"âœ“ ç”Ÿæˆäº† {len(payload2['ruleDefinition']['ruleContent'])} ä¸ªè§„åˆ™æ­¥éª¤")

        # æ˜¾ç¤ºè§„åˆ™å†…å®¹æ‘˜è¦
        for i, step in enumerate(payload2['ruleDefinition']['ruleContent'], 1):
            print(f"  æ­¥éª¤{i}: {step.get('type', 'UNKNOWN')}")

        # æµ‹è¯•ç”¨ä¾‹3ï¼šå¤æ‚ä¸šåŠ¡è§„åˆ™
        print("\nç”Ÿæˆæµ‹è¯•ç”¨ä¾‹3ï¼šé»‘è‰²é‡‘å±æ”¯å‡ºTOP3è§„åˆ™")
        payload3 = generator.create_decision_knowledge_payload(
            id="black-metal-expense-003",
            name="é»‘è‰²é‡‘å±æ”¯å‡ºTOP3",
            ruleDescription="ç­›é€‰å‡ºä¸šåŠ¡ç±»å‹ä¸º'æ”¯å‡º/ç»è¥/ç»è¥æ”¯å‡ºï¼ˆé»‘è‰²é‡‘å±ï¼‰'çš„äº¤æ˜“è®°å½•ï¼ŒæŒ‰äº¤æ˜“å¯¹æ‰‹åˆ†ç»„ï¼Œè®¡ç®—æ€»æ”¯å‡ºé‡‘é¢ï¼Œé™åºæ’åºå–å‰3å"
        )

        # ä¿å­˜JSONæ–‡ä»¶
        filename3 = "generated_rule_black_metal_expense.json"
        with open(filename3, 'w', encoding='utf-8') as f:
            json.dump(payload3, f, ensure_ascii=False, indent=2)

        print(f"âœ“ JSONæ–‡ä»¶å·²ä¿å­˜: {filename3}")
        print(f"âœ“ ç”Ÿæˆäº† {len(payload3['ruleDefinition']['ruleContent'])} ä¸ªè§„åˆ™æ­¥éª¤")

        # æ˜¾ç¤ºè§„åˆ™å†…å®¹æ‘˜è¦
        for i, step in enumerate(payload3['ruleDefinition']['ruleContent'], 1):
            print(f"  æ­¥éª¤{i}: {step.get('type', 'UNKNOWN')}")

        print("\nğŸ‰ æ‰€æœ‰JSONæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
        print(f"   - {filename1}")
        print(f"   - {filename2}")
        print(f"   - {filename3}")
        print("\nğŸ’¡ æ‚¨å¯ä»¥æ‰“å¼€è¿™äº›JSONæ–‡ä»¶æŸ¥çœ‹ç”Ÿæˆçš„å®Œæ•´è§„åˆ™å†…å®¹")

        return True

    except Exception as e:
        print(f"âœ— ç”Ÿæˆå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='è§„åˆ™ç”Ÿæˆå™¨æµ‹è¯•è„šæœ¬')
    parser.add_argument('--batch', '-b', type=str, help='æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šæŒ‡å®šåŒ…å«æŒ‡æ ‡å®šä¹‰çš„CSVæˆ–Excelæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--workers', '-w', type=int, default=3, help='å¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼Œé¿å…APIé™æµï¼‰')
    parser.add_argument('--skip-api', action='store_true', help='è·³è¿‡APIä¿å­˜ï¼Œåªç”ŸæˆJSONæ–‡ä»¶')
    parser.add_argument('--legacy', '-l', action='store_true', help='è¿è¡Œä¼ ç»Ÿæµ‹è¯•æ¨¡å¼ï¼ˆç”Ÿæˆç¤ºä¾‹è§„åˆ™ï¼‰')

    args = parser.parse_args()

    if args.batch:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        print(f"ğŸš€ å¯åŠ¨æ‰¹é‡å¤„ç†æ¨¡å¼")
        print(f"ğŸ“ æ–‡ä»¶: {args.batch}")
        print(f"âš¡ å¹¶å‘æ•°: {args.workers}")
        print(f"ğŸ’¾ è·³è¿‡API: {args.skip_api}")

        success = generate_metrics_batch(
            file_path=args.batch,
            max_workers=args.workers,
            skip_api_save=args.skip_api
        )
        if success:
            print("\nâœ… æ‰¹é‡æŒ‡æ ‡è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        else:
            print("\nâŒ æ‰¹é‡æŒ‡æ ‡è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")
            sys.exit(1)
    else:
        # é»˜è®¤ä¼ ç»Ÿæµ‹è¯•æ¨¡å¼
        success = generate_and_save_json()
        if success:
            print("\nâœ… JSONæ–‡ä»¶ç”ŸæˆæˆåŠŸï¼")
        else:
            print("\nâŒ JSONæ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼")
            sys.exit(1)


if __name__ == "__main__":
    main()
